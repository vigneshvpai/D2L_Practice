{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765e611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ace176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='mps'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def num_gpus():  #@save\n",
    "    \"\"\"Return 1 if MPS (Apple Silicon GPU) is available, else 0.\"\"\"\n",
    "    return 1 if torch.backends.mps.is_available() else 0\n",
    "\n",
    "def cpu():  #@save\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def gpu():  #@save\n",
    "    \"\"\"Get the MPS device if available, else fallback to CPU.\"\"\"\n",
    "    return torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "# Example usage\n",
    "cpu(), gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f304bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():  #@save\n",
    "    \"\"\"Return MPS device if available, else CPU.\"\"\"\n",
    "    return gpu()\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return [MPS] if available, else [CPU].\"\"\"\n",
    "    return [gpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26236eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='mps'), [device(type='mps')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu(), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c78fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='mps:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f950c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigneshvpai/miniforge3/envs/D2L/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2529c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3258],\n",
       "        [0.3258]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e486422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8caa7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "    self.save_hyperparameters()\n",
    "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    if self.gpus:\n",
    "        batch = [a.to(self.gpus[0]) for a in batch]\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_model(self, model):\n",
    "    model.trainer = self\n",
    "    model.board.xlim = [0, self.max_epochs]\n",
    "    if self.gpus:\n",
    "        model.to(self.gpus[0])\n",
    "    self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238005b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2L",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
